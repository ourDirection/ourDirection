{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "__author__ = 'Oswaldo Ludwig'\n",
    "__version__ = '1.01'\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)  # for reproducibility\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import itertools\n",
    "import operator\n",
    "import pickle\n",
    "import numpy as np    \n",
    "from keras.preprocessing import sequence\n",
    "from scipy import sparse, io\n",
    "from numpy.random import permutation\n",
    "import re\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, RepeatVector, Bidirectional, Dropout, merge\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "# import theano.tensor as T\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_hansard  = pd.read_csv('../../minerva/data/hansard_full.csv')\n",
    "\n",
    "df_group = df_hansard.groupby('subjectOfBusinessId')\n",
    "\n",
    "# for i in range(20):\n",
    "#     print(i, df_hansard.iloc[i]['content'])\n",
    "#     print('-' * 50)\n",
    "\n",
    "q_a = []\n",
    "for i, index in df_group.groups.items():\n",
    "#     print(i, index)\n",
    "    # don't bother with odd pairs\n",
    "    if (len(index) % 2 != 0): \n",
    "        continue\n",
    "        \n",
    "#     print(list(index))\n",
    "    t = df_hansard.iloc[list(index)]['content'].values\n",
    "#     print(t)\n",
    "#     print(list(zip(t[::2], t[1::2])))\n",
    "    q_a.append(list(zip(t[::2], t[1::2])))\n",
    "\n",
    "q_a = [item for sublist in q_a for item in sublist]\n",
    "\n",
    "q_a = q_a[:2000]\n",
    "\n",
    "print('number of q & a', len(q_a))\n",
    "    \n",
    "df_q_a = pd.DataFrame(q_a)\n",
    "df_q_a.columns = ['Q', 'A']\n",
    "df_q_a.to_csv('data/q_a.csv')\n",
    "df_q_a.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "questions_file = 'context'\n",
    "answers_file = 'answers'\n",
    "vocabulary_file = 'vocabulary_movie'\n",
    "padded_questions_file = 'Padded_context'\n",
    "padded_answers_file = 'Padded_answers'\n",
    "unknown_token = 'something'\n",
    "\n",
    "# vocabulary_size = 7000\n",
    "# max_features = vocabulary_size\n",
    "# maxlen_input = 50\n",
    "# maxlen_output = 50  # cut texts after this number of words\n",
    "\n",
    "print (\"Reading the context data...\")\n",
    "# q = open(questions_file, 'r')\n",
    "questions = df_q_a['Q'].values\n",
    "print (\"Reading the answer data...\")\n",
    "# a = open(answers_file, 'r')\n",
    "answers = df_q_a['A'].values\n",
    "\n",
    "all_q_a = answers + questions\n",
    "print (\"Tokenazing the answers...\")\n",
    "paragraphs_a =  answers #[p for p in answers.split('\\n')]\n",
    "paragraphs_b =  questions # all_q_a #[p for p in all.split('\\n')]\n",
    "paragraphs_a = ['BOS '+p+' EOS' for p in paragraphs_a]\n",
    "paragraphs_b = ['BOS '+p+' EOS' for p in paragraphs_b]\n",
    "\n",
    "# update DF\n",
    "df_q_a['paragraphs_a'] = paragraphs_a\n",
    "df_q_a['paragraphs_b'] = paragraphs_b\n",
    "df_q_a['Q_A'] = df_q_a[['paragraphs_a', 'paragraphs_b']].apply(lambda x: u' '.join(x), axis=1)\n",
    "\n",
    "\n",
    "# paragraphs_b = ' '.join(paragraphs_b)\n",
    "# tokenized_text = paragraphs_b.split()\n",
    "# paragraphs_q = questions #[p for p in questions.split('\\n') ]\n",
    "tokenized_answers = [p.split() for p in paragraphs_a]\n",
    "tokenized_questions = [p.split() for p in paragraphs_b]\n",
    "\n",
    "\n",
    "\n",
    "### Counting the word frequencies:\n",
    "##word_freq = nltk.FreqDist(itertools.chain(tokenized_text))\n",
    "##print (\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "##\n",
    "### Getting the most common words and build index_to_word and word_to_index vectors:\n",
    "##vocab = word_freq.most_common(vocabulary_size-1)\n",
    "##\n",
    "### Saving vocabulary:\n",
    "##with open(vocabulary_file, 'w') as v:\n",
    "##    pickle.dump(vocab, v)\n",
    "\n",
    "print(answers[0])\n",
    "print(tokenized_answers[0])\n",
    "print(questions[0])\n",
    "print(tokenized_questions[0])\n",
    "\n",
    "df_q_a.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_words = 10000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 100 \n",
    "\n",
    "# print(u' '.join(str(v) for v in tokenized_questions))\n",
    "\n",
    "print('Loading data...')\n",
    "print('number of Q and A', len(df_q_a))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df_q_a[['Q_A']].apply(lambda x: u' '.join(x))) # fit on Q and A\n",
    "X = tokenizer.texts_to_sequences(u' '.join(v) for v in tokenized_questions)\n",
    "Y = tokenizer.texts_to_sequences(u' '.join(v) for v in tokenized_answers)\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print (\"Using vocabulary of size %d.\" % max_words)\n",
    "\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "Y = pad_sequences(Y, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "print('X.shape', X.shape)\n",
    "print('Y.shape', Y.shape)\n",
    "\n",
    "print(X[0])\n",
    "\n",
    "# Replacing all words not in our vocabulary with the unknown token:\n",
    "# for i, sent in enumerate(tokenized_answers):\n",
    "#     tokenized_answers[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "# for i, sent in enumerate(tokenized_questions):\n",
    "#     tokenized_questions[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "# Creating the training data:\n",
    "# X = np.asarray([[word_to_index[w] for w in sent] for sent in tokenized_questions])\n",
    "# Y = np.asarray([[word_to_index[w] for w in sent] for sent in tokenized_answers])\n",
    "\n",
    "# Q = sequence.pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# A = sequence.pad_sequences(Y, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "# print(Q[0])\n",
    "# print(A[0])\n",
    "\n",
    "with open(padded_questions_file, 'wb') as q:\n",
    "    pickle.dump(X, q)\n",
    "    \n",
    "with open(padded_answers_file, 'wb') as a:\n",
    "    pickle.dump(Y, a)\n",
    "    \n",
    "    \n",
    "# Important.... print out the X and Y to find the EOS === here its 12\n",
    "BOS = 11\n",
    "EOS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********************************************************************\n",
    "# Reading a pre-trained word embedding and addapting to our vocabulary:\n",
    "# **********************************************************************\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open( 'data/vectors.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('normalizing vectors')\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "embedding_matrix = min_max_scaler.fit_transform(embedding_matrix)\n",
    "# embedding_matrix = preprocessing.robust_scale(embedding_matrix)\n",
    "# embedding_matrix = preprocessing.normalize(embedding_matrix)\n",
    "\n",
    "print(embedding_matrix.min(), embedding_matrix.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras model of the chatbot: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentence_embedding_size = 300\n",
    "weights_file = 'myModel'\n",
    "\n",
    "def init_model():\n",
    "    print(type(X), type(Y)) \n",
    "\n",
    "    input_context = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input_context')\n",
    "    input_answer = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input_answer')\n",
    "\n",
    "    LSTM_encoder = LSTM(sentence_embedding_size, kernel_initializer='lecun_uniform', name='encode_context')\n",
    "    LSTM_decoder = LSTM(sentence_embedding_size, kernel_initializer='lecun_uniform', name='decode_answer')\n",
    "\n",
    "    if os.path.isfile(weights_file):\n",
    "        Shared_Embedding = Embedding(output_dim=EMBEDDING_DIM, input_dim=len(word_index) + 1, \n",
    "                                     input_length=MAX_SEQUENCE_LENGTH, name='shared_embedding')\n",
    "    else:\n",
    "    #     Shared_Embedding = Embedding(output_dim=EMBEDDING_DIM, input_dim=max_words, \n",
    "    #                                  weights=[embedding_matrix], input_length=maxlen_input)\n",
    "        Shared_Embedding = Embedding(len(word_index) + 1, EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False, name='shared_embedding')\n",
    "\n",
    "    print('MAX_SEQUENCE_LENGTH', MAX_SEQUENCE_LENGTH)\n",
    "    print('max_words', max_words)\n",
    "\n",
    "    word_embedding_context = Shared_Embedding(input_context)\n",
    "    context_embedding = LSTM_encoder(word_embedding_context)\n",
    "\n",
    "    word_embedding_answer = Shared_Embedding(input_answer)\n",
    "    answer_embedding = LSTM_decoder(word_embedding_answer)\n",
    "\n",
    "    # merge_layer = Concatenate([context_embedding, answer_embedding], concat_axis=1)\n",
    "    merge_layer = Concatenate(axis=1)([context_embedding, answer_embedding])\n",
    "\n",
    "    out = Dense(int(max_words/2), activation='relu', name='relu_activation')(merge_layer)\n",
    "    out = Dense(max_words, activation='softmax', name='softmax_activation')(out)\n",
    "\n",
    "    model = Model(inputs=[input_context, input_answer], outputs=[out])\n",
    "\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.00005))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "#     if os.path.isfile(weights_file):\n",
    "#         model.load_weights(weights_file)\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = None\n",
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************************************************\n",
    "# Loading the data:\n",
    "# ************************************************************************\n",
    "n_test = 500\n",
    "num_subsets = 5\n",
    "\n",
    "q = X #cPickle.load(open(questions_file, 'rb'))\n",
    "a = Y #cPickle.load(open(answers_file, 'rb'))\n",
    "\n",
    "print(type(q))\n",
    "print(a.shape)\n",
    "# n_exem, n_words = a.shape\n",
    "n_exem = a.shape[0]\n",
    "\n",
    "qt = q[0:n_test,:]\n",
    "at = a[0:n_test,:]\n",
    "q = q[n_test + 1:,:]\n",
    "a = a[n_test + 1:,:]\n",
    "\n",
    "print(type(q))\n",
    "\n",
    "print('Number of exemples = %d'%(n_exem - n_test))\n",
    "print('qt', qt.shape, 'q', q.shape)\n",
    "print('at', at.shape, 'a', a.shape)\n",
    "step = int(np.around((n_exem - n_test)/num_subsets))\n",
    "round_exem = int(step * num_subsets)\n",
    "print('step', step, 'round_exem', round_exem)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vocabulary = dict((v, k) for k, v in tokenizer.word_index.items())\n",
    "# vocabulary[1]\n",
    "\n",
    "\n",
    "\n",
    "def print_result(input_text):\n",
    "\n",
    "    ans_partial = np.zeros((1,MAX_SEQUENCE_LENGTH))\n",
    "    ans_partial[0, -1] = BOS  #  the index of the symbol BOS (begin of sentence)\n",
    "    for k in range(MAX_SEQUENCE_LENGTH - 1):\n",
    "        ye = model.predict([input_text, ans_partial])\n",
    "        mp = np.argmax(ye)\n",
    "        ans_partial[0, 0:-1] = ans_partial[0, 1:]\n",
    "        ans_partial[0, -1] = mp\n",
    "    \n",
    "#     print(ans_partial)\n",
    "    text = ''\n",
    "    for k in ans_partial[0]:\n",
    "        k = k.astype(int)\n",
    "        if k < (max_words-2):\n",
    "            w = vocabulary[k]\n",
    "            text = text + w + ' '\n",
    "    return(text)\n",
    "\n",
    "def print_index_to_vocab(input_index):\n",
    "    text = ''\n",
    "    input_index = filter(lambda a: a != 0, input_index)\n",
    "    for k in input_index:\n",
    "        text = text + vocabulary[k] + ' '\n",
    "    return(text)\n",
    "\n",
    "    \n",
    "\n",
    "# print(qt[0])\n",
    "# print(at[0])\n",
    "# print(q[0])\n",
    "# print(a[0])\n",
    "\n",
    "# print(print_index_to_vocab(qt[41]))\n",
    "# test_input = at[41:42]\n",
    "# print(print_result(test_input))\n",
    "\n",
    "\n",
    "# train_input = q[41:42]\n",
    "# print(print_result(train_input)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************************************************************\n",
    "# Bot training:\n",
    "# *************************************************************************\n",
    "Epochs = 100\n",
    "BatchSize = 128  #  Check the capacity of your GPU\n",
    "Patience = 0\n",
    "dropout = .25\n",
    "# n_test = 100\n",
    "\n",
    "\n",
    "print(round_exem, step)\n",
    "\n",
    "x = range(0,Epochs) \n",
    "valid_loss = np.zeros(Epochs)\n",
    "train_loss = np.zeros(Epochs)\n",
    "for m in range(Epochs):\n",
    "    \n",
    "    # Loop over training batches due to memory constraints:\n",
    "    for n in range(0,round_exem,step):\n",
    "        print('n', n)\n",
    "        \n",
    "        q2 = q[n:n+step]\n",
    "        s = q2.shape\n",
    "        count = 0\n",
    "        for i, sent in enumerate(a[n:n+step]):\n",
    "            l = np.where(sent==EOS)  #  the position od the symbol EOS\n",
    "            limit = l[0][0]\n",
    "            count += limit + 1\n",
    "            \n",
    "        print(count)\n",
    "        Q = np.zeros((count,MAX_SEQUENCE_LENGTH))\n",
    "        A = np.zeros((count,MAX_SEQUENCE_LENGTH))\n",
    "        Y = np.zeros((count,max_words))\n",
    "        \n",
    "        # Loop over the training examples:\n",
    "        count = 0\n",
    "        for i, sent in enumerate(a[n:n+step]):\n",
    "            ans_partial = np.zeros((1,MAX_SEQUENCE_LENGTH))\n",
    "            \n",
    "            # Loop over the positions of the current target output (the current output sequence):\n",
    "            l = np.where(sent==EOS)  #  the position of the symbol EOS\n",
    "            limit = l[0][0]\n",
    "\n",
    "            for k in range(1,limit+1):\n",
    "                # Mapping the target output (the next output word) for one-hot codding:\n",
    "                y = np.zeros((1, max_words))\n",
    "                y[0, sent[k]] = 1\n",
    "\n",
    "                # preparing the partial answer to input:\n",
    "\n",
    "                ans_partial[0,-k:] = sent[0:k]\n",
    "\n",
    "                # training the model for one epoch using teacher forcing:\n",
    "                \n",
    "                Q[count, :] = q2[i:i+1] \n",
    "                A[count, :] = ans_partial \n",
    "                Y[count, :] = y\n",
    "                count += 1\n",
    "                \n",
    "        print('Training epoch: %d, training examples: %d - %d'%(m,n, n + step))\n",
    "        model.fit([Q, A], Y, batch_size=BatchSize, epochs=1, verbose=1)\n",
    "         \n",
    "        test_input = qt[41:42]\n",
    "        print(print_result(test_input))\n",
    "        train_input = q[41:42]\n",
    "        print(print_result(train_input))        \n",
    "        \n",
    "    model.save_weights(weights_file, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
