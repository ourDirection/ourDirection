{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonic/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import unicodedata\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the movie conversation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data from here - https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html  or\n",
    "\n",
    "## run pull.sh in data folder to fetch the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-03-08 00:32:57--  https://www.dropbox.com/s/ncfa5t950gvtaeb/test.enc?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.1, 2620:100:601c:1::a27d:601\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/FjAUszSXOEj1H0pWzR5jh44IliaQkdoNff37xn0FBpetcMjXsdXoyhrocWmwxC8X/file [following]\n",
      "--2018-03-08 00:32:57--  https://dl.dropboxusercontent.com/content_link/FjAUszSXOEj1H0pWzR5jh44IliaQkdoNff37xn0FBpetcMjXsdXoyhrocWmwxC8X/file\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.6.6, 2620:100:601c:6::a27d:606\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.6.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n",
      "--2018-03-08 00:32:58--  https://www.dropbox.com/s/48ro4759jaikque/test.dec?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.1, 2620:100:601c:1::a27d:601\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/YZcVFiAr4X1dqUEkbhxBOYP4lqpa04L3zMKVwlagGzUrLSr5dirAoCIBum2gJcrI/file [following]\n",
      "--2018-03-08 00:32:58--  https://dl.dropboxusercontent.com/content_link/YZcVFiAr4X1dqUEkbhxBOYP4lqpa04L3zMKVwlagGzUrLSr5dirAoCIBum2gJcrI/file\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.6.6, 2620:100:601c:6::a27d:606\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.6.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n",
      "--2018-03-08 00:32:59--  https://www.dropbox.com/s/gu54ngk3xpwite4/train.enc?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.1, 2620:100:601c:1::a27d:601\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/CYAMohUsD6l7imzzPT44zD76RYTN8YmecyrIvCd7bhqx3be7Fd4aJ1LK0vH6idUA/file [following]\n",
      "--2018-03-08 00:33:00--  https://dl.dropboxusercontent.com/content_link/CYAMohUsD6l7imzzPT44zD76RYTN8YmecyrIvCd7bhqx3be7Fd4aJ1LK0vH6idUA/file\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.6.6, 2620:100:601c:6::a27d:606\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.6.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n",
      "--2018-03-08 00:33:01--  https://www.dropbox.com/s/g3z2msjziqocndl/train.dec?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.1, 2620:100:601c:1::a27d:601\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/m9JnWVicp6fgx20ScENcn2TtPzuvrN25I5QoW4GtYiE187iczBQzMHiXfZcymUmz/file [following]\n",
      "--2018-03-08 00:33:02--  https://dl.dropboxusercontent.com/content_link/m9JnWVicp6fgx20ScENcn2TtPzuvrN25I5QoW4GtYiE187iczBQzMHiXfZcymUmz/file\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.6.6, 2620:100:601c:6::a27d:606\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.6.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! data/pull.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.dec', 'train.enc', 'test.dec', 'conversation.pickle', 'pull.sh', 'test.enc']\n"
     ]
    }
   ],
   "source": [
    "path='data/'\n",
    "print (os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc=[]\n",
    "train_dec=[]\n",
    "f1=open('data/train.enc','rb')\n",
    "f2=open('data/train.dec','rb')\n",
    "for i in f1.readlines():\n",
    "    try:\n",
    "        temp1=i.decode(\"utf-8\",errors='replace')\n",
    "#         print (temp1[0])\n",
    "    except Exception as e:\n",
    "        print ('***********')\n",
    "        print (i)\n",
    "        break\n",
    "    train_enc.append(temp1)\n",
    "    \n",
    "for j in f2.readlines():\n",
    "    try:\n",
    "        temp2=j.decode(\"utf-8\",errors='replace')\n",
    "#         print (temp2[0])\n",
    "    except Exception as e:\n",
    "        print ('***********')\n",
    "        print (i)\n",
    "        break\n",
    "    train_dec.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_enc=[]\n",
    "test_dec=[]\n",
    "f1=open('data/test.enc','rb')\n",
    "f2=open('data/test.dec','rb')\n",
    "for i in f1.readlines():\n",
    "    try:\n",
    "        temp1=i.decode(\"utf-8\",errors='replace')\n",
    "#         print (temp1[0])\n",
    "    except Exception as e:\n",
    "        print ('***********')\n",
    "        print (i)\n",
    "        break\n",
    "    test_enc.append(temp1)\n",
    "    \n",
    "for j in f2.readlines():\n",
    "    try:\n",
    "        temp2=j.decode(\"utf-8\",errors='replace')\n",
    "#         print (temp2[0])\n",
    "    except Exception as e:\n",
    "        print ('***********')\n",
    "        print (i)\n",
    "        break\n",
    "    test_dec.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\n', \"You're asking me out.  That's so cute. What's your name again?\\n\"] [\"Well, I thought we'd start with pronunciation, if that's okay with you.\\n\", 'Forget it.\\n']\n"
     ]
    }
   ],
   "source": [
    "print (train_enc[0:2],train_dec[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list=[]\n",
    "for i,j in zip(train_enc,train_dec):\n",
    "    train_list.append([i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person1</th>\n",
       "      <th>person2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>What the hell is that?\\n</td>\n",
       "      <td>That's the Core: the gravity drive. The heart ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>The safety circuit's failed!\\n</td>\n",
       "      <td>We're losing atmosphere...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>I didn't see anything and I don't have to see ...</td>\n",
       "      <td>Thank you for that scientific analysis, Mister...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>I can't believe this, I haven't gotten more th...</td>\n",
       "      <td>Smith's right. Neptune? There's nothing out th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>30 hours to Neptune orbit.\\n</td>\n",
       "      <td>All boards are green, everything's five by fiv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                person1  \\\n",
       "9995                           What the hell is that?\\n   \n",
       "9996                     The safety circuit's failed!\\n   \n",
       "9997  I didn't see anything and I don't have to see ...   \n",
       "9998  I can't believe this, I haven't gotten more th...   \n",
       "9999                       30 hours to Neptune orbit.\\n   \n",
       "\n",
       "                                                person2  \n",
       "9995  That's the Core: the gravity drive. The heart ...  \n",
       "9996                       We're losing atmosphere...\\n  \n",
       "9997  Thank you for that scientific analysis, Mister...  \n",
       "9998  Smith's right. Neptune? There's nothing out th...  \n",
       "9999  All boards are green, everything's five by fiv...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.DataFrame(train_list,columns=['person1','person2'])\n",
    "data = data[:10000]\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For doing preprocessing of text you can alter below code to work on both person1 and person2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "import re\n",
    "import numpy as np\n",
    "# #stop=['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}','/','-']\n",
    "# def number_removal(row):\n",
    "#     data1 = row['Summary']\n",
    "#     #print data1\n",
    "#     #tokens = nltk.wordpunct_tokenize(data)\n",
    "#     #print (type(data1))\n",
    "#     if type(data1) not in [int,float]:\n",
    "#         print type(data1)\n",
    "#         line = re.sub(r\"[^A-Za-z\\s]\", \" \", data1.strip())\n",
    "#         tokens = line.split()\n",
    "#     else:\n",
    "#         tokens=[]\n",
    "#     return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# frequency_words_wo_stop = {}\n",
    "# def generate_word_frequency(row):\n",
    "#     data1 = row['Summary']\n",
    "#     tokens = nltk.wordpunct_tokenize(data1)\n",
    "#     token_list = []\n",
    "#     for token in tokens:\n",
    "# #         if token.lower() not in stop:\n",
    "# #             token_list.append(token.lower())\n",
    "#         if token.lower() in frequency_words_wo_stop:\n",
    "#             count = frequency_words_wo_stop[token.lower()]\n",
    "#             count = count + 1\n",
    "#             frequency_words_wo_stop[token.lower()] = count\n",
    "#         else:\n",
    "#             frequency_words_wo_stop[token.lower()] = 1\n",
    "    \n",
    "#     return ','.join(token_list)\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# def receieve(query):\n",
    "#     #data=pd.DataFrame([query])\n",
    "#     #data.columns=['Summary']\n",
    "#     data1=query\n",
    "#     #print (data1.head())\n",
    "#     data1['Summary'] = data1.apply(number_removal,axis=1)\n",
    "    \n",
    "#     data1['tokens'] = data1.apply(generate_word_frequency,axis=1)\n",
    "#     print (data1.head())\n",
    "#     big=[]\n",
    "#     for i in data1['tokens']:\n",
    "#         st=''\n",
    "#         ls=[]\n",
    "#         for j in i.split(','):\n",
    "#             #print j\n",
    "#             print (wordnet_lemmatizer.lemmatize(j))\n",
    "#             ls.append(wordnet_lemmatizer.lemmatize(j))\n",
    "#         #print ls\n",
    "#         big.append(' '.join(ls))\n",
    "#     data1['Summary_lem']=big\n",
    "#     return data1['Summary_lem']\n",
    "# data['Summary']=receieve(data[['Summary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nltk.stem import WordNetLemmatizer\n",
    "# wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# def fun(dat):\n",
    "#     big=[]\n",
    "#     for i in dat['Question']:\n",
    "#         st=''\n",
    "#         ls=[]\n",
    "#         for j in i.split():\n",
    "#             #print j\n",
    "#             ls.append(wordnet_lemmatizer.lemmatize(re.sub(r\"[^A-Za-z\\s]\", \" \", j.strip().lower())))\n",
    "#         #print ls\n",
    "#         big.append(' '.join(ls))\n",
    "#     return big\n",
    "\n",
    "\n",
    "# data['Question']=fun(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=data['person1'].values\n",
    "y=data['person2'].values\n",
    "\n",
    "# skf = StratifiedKFold(y, n_folds=4)\n",
    "# for train_index, test_index in skf:\n",
    "#     #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert words into vectors using Google's word vectors\n",
    "### Download from https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sonic/sonic/ourDirection/momus/NLG-LSTM-Keras\r\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "# model = gensim.models.Word2Vec.load('../data/5', binary='binary')\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok_x=[]\n",
    "tok_y=[]\n",
    "for i in range(len(X)):\n",
    "    tok_x.append(nltk.word_tokenize(X[i].lower()))\n",
    "    tok_y.append(nltk.word_tokenize(y[i].lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['well',\n",
       "  ',',\n",
       "  'i',\n",
       "  'thought',\n",
       "  'we',\n",
       "  \"'d\",\n",
       "  'start',\n",
       "  'with',\n",
       "  'pronunciation',\n",
       "  ',',\n",
       "  'if',\n",
       "  'that',\n",
       "  \"'s\",\n",
       "  'okay',\n",
       "  'with',\n",
       "  'you',\n",
       "  '.'],\n",
       " ['forget', 'it', '.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_y[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentend=np.ones((300,),dtype=np.float32) \n",
    "\n",
    "vec_x=[]\n",
    "for sent in tok_x:\n",
    "    sentvec = [model[w] for w in sent if w in model.vocab]\n",
    "    vec_x.append(sentvec)\n",
    "    \n",
    "vec_y=[]\n",
    "for sent in tok_y:\n",
    "    sentvec = [model[w] for w in sent if w in model.vocab]\n",
    "    vec_y.append(sentvec)           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tok_sent in vec_x:\n",
    "    tok_sent[14:]=[]\n",
    "    tok_sent.append(sentend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tok_sent in vec_x:\n",
    "    if len(tok_sent)<15:\n",
    "        for i in range(15-len(tok_sent)):\n",
    "            tok_sent.append(sentend) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(tok_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tok_sent in vec_y:\n",
    "    tok_sent[14:]=[]\n",
    "    tok_sent.append(sentend)\n",
    "    \n",
    "\n",
    "for tok_sent in vec_y:\n",
    "    if len(tok_sent)<15:\n",
    "        for i in range(15-len(tok_sent)):\n",
    "            tok_sent.append(sentend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/conversation.pickle','wb') as f:\n",
    "    pickle.dump([vec_x,vec_y],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM,SimpleRNN\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open('data/conversation.pickle','rb')\n",
    "vec_x,vec_y=pickle.load(f)    \n",
    "    \n",
    "vec_x=np.array(vec_x,dtype=np.float32)\n",
    "vec_y=np.array(vec_y,dtype=np.float32)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train,x_test, y_train,y_test = train_test_split(vec_x, vec_y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 15, 300)           721200    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 15, 300)           721200    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 15, 300)           721200    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 15, 300)           721200    \n",
      "=================================================================\n",
      "Total params: 2,884,800\n",
      "Trainable params: 2,884,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(LSTM(units=300, input_shape=x_train.shape[1:],\n",
    "               return_sequences=True, recurrent_initializer='glorot_normal', kernel_initializer='glorot_normal', activation='sigmoid'))\n",
    "model.add(LSTM(units=300,input_shape=x_train.shape[1:],\n",
    "               return_sequences=True, recurrent_initializer='glorot_normal', kernel_initializer='glorot_normal', activation='sigmoid'))\n",
    "model.add(LSTM(units=300,input_shape=x_train.shape[1:],\n",
    "               return_sequences=True, recurrent_initializer='glorot_normal', kernel_initializer='glorot_normal', activation='sigmoid'))\n",
    "model.add(LSTM(units=300,input_shape=x_train.shape[1:],\n",
    "               return_sequences=True, recurrent_initializer='glorot_normal', kernel_initializer='glorot_normal', activation='sigmoid'))\n",
    "model.compile(loss='cosine_proximity', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/500\n",
      "8000/8000 [==============================] - 123s 15ms/step - loss: -0.5895 - acc: 0.0266 - val_loss: -0.5953 - val_acc: 0.0271\n",
      "Epoch 2/500\n",
      "8000/8000 [==============================] - 124s 15ms/step - loss: -0.5898 - acc: 0.0272 - val_loss: -0.5957 - val_acc: 0.0235\n",
      "Epoch 3/500\n",
      "8000/8000 [==============================] - 123s 15ms/step - loss: -0.5901 - acc: 0.0271 - val_loss: -0.5956 - val_acc: 0.0247\n",
      "Epoch 4/500\n",
      "8000/8000 [==============================] - 124s 16ms/step - loss: -0.5904 - acc: 0.0281 - val_loss: -0.5965 - val_acc: 0.0263\n",
      "Epoch 5/500\n",
      "3040/8000 [==========>...................] - ETA: 1:15 - loss: -0.5884 - acc: 0.0288"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=500, validation_data=(x_test, y_test))\n",
    "model.save('data/LSTM500.h5')\n",
    "# model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "# model.save('lstm_models/LSTM1000.h5')\n",
    "# model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "# model.save('lstm_models/LSTM1500.h5')\n",
    "# model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "# model.save('lstm_models/LSTM2000.h5')\n",
    "# model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "# model.save('lstm_models/LSTM2500.h5')\n",
    "# model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "# model.save('lstm_models/LSTM3000.h5')\n",
    "# model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "# model.save('lstm_models/LSTM3500.h5')\n",
    "# model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "# model.save('lstm_models/LSTM4000.h5')\n",
    "# model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "# model.save('lstm_models/LSTM4500.h5')\n",
    "# model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "# model.save('lstm_models/LSTM5000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7999, 15, 300)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Laughing.', 0.5424548983573914),\n",
       " ('do', 0.5389987230300903),\n",
       " ('anyway', 0.5267603993415833),\n",
       " ('RUSH_Okay', 0.5002189874649048),\n",
       " ('Stupid_stupid', 0.46662095189094543),\n",
       " ('Stupid_stupid', 0.43449124693870544),\n",
       " ('reallllly', 0.3895082473754883),\n",
       " ('reallllly', 0.334981769323349),\n",
       " ('reallllly', 0.311807245016098),\n",
       " ('reallllly', 0.29463836550712585),\n",
       " ('reallllly', 0.27858787775039673),\n",
       " ('reallllly', 0.26468509435653687),\n",
       " ('Australian_sheepskin_boots', 0.2539001703262329),\n",
       " ('Australian_sheepskin_boots', 0.25108352303504944),\n",
       " ('Australian_sheepskin_boots', 0.24872171878814697)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=model.predict(x_test) \n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)   \n",
    "[w2v_model.most_similar([predictions[10][i]])[0] for i in range(15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x='what this all about'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laughing. do anyway RUSH_Okay going Stupid_stupid reallllly reallllly reallllly reallllly reallllly reallllly Australian_sheepskin_boots Australian_sheepskin_boots Australian_sheepskin_boots\n"
     ]
    }
   ],
   "source": [
    "sentend=np.ones((300,),dtype=np.float32) \n",
    "\n",
    "sent=nltk.word_tokenize(x.lower())\n",
    "sentvec = [w2v_model[w] for w in sent if w in w2v_model.vocab]\n",
    "\n",
    "sentvec[14:]=[]\n",
    "sentvec.append(sentend)\n",
    "if len(sentvec)<15:\n",
    "    for i in range(15-len(sentvec)):\n",
    "        sentvec.append(sentend) \n",
    "sentvec=np.array([sentvec])\n",
    "\n",
    "predictions = model.predict(sentvec)\n",
    "#print (predictions)\n",
    "outputlist=[w2v_model.most_similar([predictions[0][i]])[0][0] for i in range(15)]\n",
    "output=' '.join(outputlist)\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.15575278,  0.        ,  0.30011603,  0.70224357,  0.        ,\n",
       "         0.06596337,  0.1735681 ,  0.        ,  0.30567577,  0.28602865,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.5       ,  0.4112463 ,  0.33293316,  0.18313767,  0.        ,\n",
       "         0.        ,  0.26286513,  0.5       ,  0.        ,  0.13168032,\n",
       "         0.15482874,  0.        ,  0.02708067,  0.10439767,  0.        ,\n",
       "         0.        ,  0.36169937,  0.        ,  0.        ,  0.        ,\n",
       "         0.07590561,  0.11171825,  0.18639541,  0.10530861,  0.5       ,\n",
       "         0.46031433,  0.        ,  0.73098242,  0.        ,  0.10333289,\n",
       "         0.11773524,  0.03792454,  0.        ,  0.03366207,  0.        ,\n",
       "         0.        ,  0.361404  ,  0.06697976,  0.14651617,  0.02948635,\n",
       "         0.22108823,  0.        ,  0.        ,  0.20671573,  0.        ,\n",
       "         0.        ,  0.42755577,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.5       ,  0.        ,  0.15188712,\n",
       "         0.5       ,  0.3500869 ,  0.28381652,  0.        ,  0.        ,\n",
       "         0.        ,  0.43476924,  0.50207484,  0.23441485,  0.66407293,\n",
       "         0.        ,  0.        ,  0.24023695,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.73099315,  0.        ,  0.01058624,\n",
       "         0.17081872,  0.5589546 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.24555546,  0.30502611,  0.18168607,  0.        ,\n",
       "         0.        ,  0.        ,  0.13908324,  0.21795085,  0.        ,\n",
       "         0.05736056,  0.        ,  0.        ,  0.13160923,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.51034981,\n",
       "         0.        ,  0.10097566,  0.        ,  0.26998571,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.33496183,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.2929852 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.01368625,  0.14929339,  0.21799757,  0.32000893,\n",
       "         0.42617258,  0.        ,  0.31937313,  0.        ,  0.17074116,\n",
       "         0.10579916,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.30450675,  0.15263778,  0.        ,  0.12118845,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.12649316,\n",
       "         0.05207963,  0.54408228,  0.        ,  0.33161491,  0.04931435,\n",
       "         0.        ,  0.32085359,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.11740716,  0.        ,  0.        ,  0.        ,\n",
       "         0.26545712,  0.6519081 ,  0.        ,  0.12714165,  0.        ,\n",
       "         0.        ,  0.        ,  0.13889204,  0.        ,  0.03345153,\n",
       "         0.03007419,  0.        ,  0.1940217 ,  0.19476694,  0.1526396 ,\n",
       "         0.22209699,  0.32185647,  0.        ,  0.        ,  0.18971442,\n",
       "         0.09272215,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.30282569,  0.        ,  0.07300915,  0.03060656,\n",
       "         0.        ,  0.        ,  0.        ,  0.04953147,  0.19884457,\n",
       "         0.09078991,  0.43079382,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.73085517,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.15217614,  0.        ,  0.        ,  0.52902007,\n",
       "         0.        ,  0.5       ,  0.26928765,  0.        ,  0.        ,\n",
       "         0.14197117,  0.        ,  0.14013539,  0.        ,  0.45105782,\n",
       "         0.07889698,  0.        ,  0.        ,  0.52157062,  0.03371609,\n",
       "         0.2177621 ,  0.08904567,  0.04480101,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.5       ,  0.        ,  0.        ,\n",
       "         0.15113029,  0.24843462,  0.50070685,  0.35635176,  0.46760482,\n",
       "         0.        ,  0.24555027,  0.        ,  0.        ,  0.        ,\n",
       "         0.04726614,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.41765285,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.11903664,  0.39583093,  0.73072612,  0.47076958,  0.22687623,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.11003927,\n",
       "         0.13898432,  0.        ,  0.18465172,  0.5       ,  0.02331829,\n",
       "         0.0264422 ,  0.        ,  0.        ,  0.        ,  0.31412211,\n",
       "         0.        ,  0.4002004 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.02296525,  0.        ,  0.        ,  0.        ], dtype=float32),\n",
       " (300,),\n",
       " (15, 300))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0][0],predictions[0][0].shape,predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[87, 87, 42, 42, 42, 42, 42, 87, 87, 87, 87, 42, 42, 42, 42]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(sentvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Laughing.', 0.5428836345672607),\n",
       " ('anyway', 0.537278950214386),\n",
       " ('THE_PRESIDENT_Yes', 0.5365479588508606),\n",
       " (\"O'REILLY_OK\", 0.534572184085846),\n",
       " ('RUSH_Okay', 0.53199303150177),\n",
       " ('Stupid_stupid', 0.5310018658638),\n",
       " ('ÔI', 0.5272243618965149),\n",
       " ('Golly_gee', 0.5240897536277771),\n",
       " ('Mr._DIONNE', 0.5185691118240356),\n",
       " ('RUSH_Yeah', 0.5185155868530273)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar([predictions[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feel free to improve it and provide feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
