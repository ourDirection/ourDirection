{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from data_iterator import *\n",
    "from state import *\n",
    "from dialog_encdec import *\n",
    "from utils import *\n",
    "\n",
    "import time\n",
    "import traceback\n",
    "import sys\n",
    "import argparse\n",
    "import pickle\n",
    "import logging\n",
    "import search\n",
    "import pprint\n",
    "import numpy\n",
    "import collections\n",
    "import signal\n",
    "import math\n",
    "import gc\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import pylab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Unbuffered:\n",
    "    def __init__(self, stream):\n",
    "        self.stream = stream\n",
    "\n",
    "    def write(self, data):\n",
    "        self.stream.write(data)\n",
    "        self.stream.flush()\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return getattr(self.stream, attr)\n",
    "\n",
    "sys.stdout = Unbuffered(sys.stdout)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "### Unique RUN_ID for this execution\n",
    "RUN_ID = str(time.time())\n",
    "\n",
    "### Additional measures can be set here\n",
    "measures = [\"train_cost\", \"train_misclass\", \"train_kl_divergence_cost\", \"train_posterior_mean_variance\", \"valid_cost\", \n",
    "            \"valid_misclass\", \"valid_posterior_mean_variance\", \"valid_kl_divergence_cost\", \"valid_emi\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_timings():\n",
    "    timings = {}\n",
    "    for m in measures:\n",
    "        timings[m] = []\n",
    "    return timings\n",
    "\n",
    "def save(model, timings, post_fix = ''):\n",
    "    print (\"Saving the model...\")\n",
    "\n",
    "    # ignore keyboard interrupt while saving\n",
    "    start = time.time()\n",
    "    s = signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
    "    \n",
    "    model.save(model.state['save_dir'] + '/' + model.state['run_id'] + \"_\" + model.state['prefix'] + post_fix + 'model.npz')\n",
    "    cPickle.dump(model.state, open(model.state['save_dir'] + '/' +  model.state['run_id'] + \"_\" + model.state['prefix'] + post_fix + 'state.pkl', 'w'))\n",
    "    numpy.savez(model.state['save_dir'] + '/' + model.state['run_id'] + \"_\" + model.state['prefix'] + post_fix + 'timing.npz', **timings)\n",
    "    signal.signal(signal.SIGINT, s)\n",
    "    \n",
    "    print (\"Model saved, took {}\".format(time.time() - start))\n",
    "\n",
    "def load(model, filename, parameter_strings_to_ignore):\n",
    "    print (\"Loading the model...\")\n",
    "\n",
    "    # ignore keyboard interrupt while saving\n",
    "    start = time.time()\n",
    "    s = signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
    "    model.load(filename, parameter_strings_to_ignore)\n",
    "    signal.signal(signal.SIGINT, s)\n",
    "\n",
    "    print (\"Model loaded, took {}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(args):     \n",
    "    logging.basicConfig(level = logging.DEBUG,\n",
    "                        format = \"%(asctime)s: %(name)s: %(levelname)s: %(message)s\")\n",
    "   \n",
    "    state = eval(args.prototype)() \n",
    "    timings = init_timings() \n",
    "\n",
    "    auto_restarting = False\n",
    "    if args.auto_restart:\n",
    "        assert not args.save_every_valid_iteration\n",
    "        assert len(args.resume) == 0\n",
    "\n",
    "        directory = state['save_dir']\n",
    "        if not directory[-1] == '/':\n",
    "            directory = directory + '/' \n",
    "\n",
    "        auto_resume_postfix = state['prefix'] + '_auto_model.npz'\n",
    "\n",
    "        if os.path.exists(directory):\n",
    "            directory_files = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "            resume_filename = ''\n",
    "            for f in directory_files:\n",
    "                if len(f) > len(auto_resume_postfix):\n",
    "                    if f[len(f) - len(auto_resume_postfix):len(f)] == auto_resume_postfix:\n",
    "                        if len(resume_filename) > 0:\n",
    "                            print ('ERROR: FOUND MULTIPLE MODELS IN DIRECTORY:', directory)\n",
    "                            assert False\n",
    "                        else:\n",
    "                            resume_filename = directory + f[0:len(f)-len('__auto_model.npz')]\n",
    "\n",
    "            if len(resume_filename) > 0:\n",
    "                logger.debug(\"Found model to automatically resume: %s\" % resume_filename)\n",
    "                auto_restarting = True\n",
    "                # Setup training to automatically resume training with the model found\n",
    "                args.resume = resume_filename + '__auto'\n",
    "                # Disable training from reinitialization any parameters\n",
    "                args.reinitialize_decoder_parameters = False\n",
    "                args.reinitialize_latent_variable_parameters = False\n",
    "            else:\n",
    "                logger.debug(\"Could not find any model to automatically resume...\")\n",
    "\n",
    "\n",
    "\n",
    "    if args.resume != \"\":\n",
    "        logger.debug(\"Resuming %s\" % args.resume)\n",
    "        \n",
    "        state_file = args.resume + '_state.pkl'\n",
    "        timings_file = args.resume + '_timing.npz'\n",
    "        \n",
    "        if os.path.isfile(state_file) and os.path.isfile(timings_file):\n",
    "            logger.debug(\"Loading previous state\")\n",
    "            \n",
    "            state = cPickle.load(open(state_file, 'r'))\n",
    "            timings = dict(numpy.load(open(timings_file, 'r')))\n",
    "            for x, y in timings.items():\n",
    "                timings[x] = list(y)\n",
    "\n",
    "            # Increment seed to make sure we get newly shuffled batches when training on large datasets\n",
    "            state['seed'] = state['seed'] + 10\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Cannot resume, cannot find files!\")\n",
    "\n",
    "\n",
    "\n",
    "    logger.debug(\"State:\\n{}\".format(pprint.pformat(state)))\n",
    "    logger.debug(\"Timings:\\n{}\".format(pprint.pformat(timings)))\n",
    " \n",
    "    if args.force_train_all_wordemb == True:\n",
    "        state['fix_pretrained_word_embeddings'] = False\n",
    "\n",
    "    model = DialogEncoderDecoder(state)\n",
    "    rng = model.rng \n",
    "\n",
    "    valid_rounds = 0\n",
    "    save_model_on_first_valid = False\n",
    "\n",
    "    if args.resume != \"\":\n",
    "        filename = args.resume + '_model.npz'\n",
    "        if os.path.isfile(filename):\n",
    "            logger.debug(\"Loading previous model\")\n",
    "\n",
    "            parameter_strings_to_ignore = []\n",
    "            if args.reinitialize_decoder_parameters:\n",
    "                parameter_strings_to_ignore += ['Wd_']\n",
    "                parameter_strings_to_ignore += ['bd_']\n",
    "\n",
    "                save_model_on_first_valid = True\n",
    "            if args.reinitialize_latent_variable_parameters:\n",
    "                parameter_strings_to_ignore += ['latent_utterance_prior']\n",
    "                parameter_strings_to_ignore += ['latent_utterance_approx_posterior']\n",
    "                parameter_strings_to_ignore += ['kl_divergence_cost_weight']\n",
    "                parameter_strings_to_ignore += ['latent_dcgm_encoder']\n",
    "\n",
    "                save_model_on_first_valid = True\n",
    "\n",
    "            load(model, filename, parameter_strings_to_ignore)\n",
    "        else:\n",
    "            raise Exception(\"Cannot resume, cannot find model file!\")\n",
    "        \n",
    "        if 'run_id' not in model.state:\n",
    "            raise Exception('Backward compatibility not ensured! (need run_id in state)')           \n",
    "\n",
    "    else:\n",
    "        # assign new run_id key\n",
    "        model.state['run_id'] = RUN_ID\n",
    "\n",
    "    logger.debug(\"Compile trainer\")\n",
    "    if not state[\"use_nce\"]:\n",
    "        if ('add_latent_gaussian_per_utterance' in state) and (state[\"add_latent_gaussian_per_utterance\"]):\n",
    "            logger.debug(\"Training using variational lower bound on log-likelihood\")\n",
    "        else:\n",
    "            logger.debug(\"Training using exact log-likelihood\")\n",
    "\n",
    "        train_batch = model.build_train_function()\n",
    "    else:\n",
    "        logger.debug(\"Training with noise contrastive estimation\")\n",
    "        train_batch = model.build_nce_function()\n",
    "\n",
    "    eval_batch = model.build_eval_function()\n",
    "\n",
    "    if model.add_latent_gaussian_per_utterance:\n",
    "        eval_grads = model.build_eval_grads()\n",
    "\n",
    "    random_sampler = search.RandomSampler(model)\n",
    "    beam_sampler = search.BeamSampler(model) \n",
    "\n",
    "    logger.debug(\"Load data\")\n",
    "    train_data, valid_data, = get_train_iterator(state)\n",
    "    train_data.start()\n",
    "\n",
    "    # Start looping through the dataset\n",
    "    step = 0\n",
    "    patience = state['patience'] \n",
    "    start_time = time.time()\n",
    "     \n",
    "    train_cost = 0\n",
    "    train_kl_divergence_cost = 0\n",
    "    train_posterior_mean_variance = 0\n",
    "    train_misclass = 0\n",
    "    train_done = 0\n",
    "    train_dialogues_done = 0.0\n",
    "\n",
    "    prev_train_cost = 0\n",
    "    prev_train_done = 0\n",
    "\n",
    "    ex_done = 0\n",
    "    is_end_of_batch = True\n",
    "    start_validation = False\n",
    "\n",
    "    batch = None\n",
    "\n",
    "    while (step < state['loop_iters'] and\n",
    "            (time.time() - start_time)/60. < state['time_stop'] and\n",
    "            patience >= 0):\n",
    "\n",
    "        ### Sampling phase\n",
    "        if step % 200 == 0:\n",
    "            # First generate stochastic samples\n",
    "            for param in model.params:\n",
    "                print (\"%s = %.4f\" % (param.name, numpy.sum(param.get_value() ** 2) ** 0.5))\n",
    "\n",
    "            samples, costs = random_sampler.sample([[]], n_samples=1, n_turns=3)\n",
    "            print (\"Sampled : {}\".format(samples[0]))\n",
    "\n",
    "\n",
    "        ### Training phase\n",
    "        batch = train_data.next()\n",
    "\n",
    "        # Train finished\n",
    "        if not batch:\n",
    "            # Restart training\n",
    "            logger.debug(\"Got None...\")\n",
    "            break\n",
    "        \n",
    "        logger.debug(\"[TRAIN] - Got batch %d,%d\" % (batch['x'].shape[1], batch['max_length']))\n",
    "        \n",
    "        x_data = batch['x']\n",
    "        x_data_reversed = batch['x_reversed']\n",
    "        max_length = batch['max_length']\n",
    "        x_cost_mask = batch['x_mask']\n",
    "        x_reset = batch['x_reset']\n",
    "        ran_cost_utterance = batch['ran_var_constutterance']\n",
    "        ran_decoder_drop_mask = batch['ran_decoder_drop_mask']\n",
    "\n",
    "        is_end_of_batch = False\n",
    "        if numpy.sum(numpy.abs(x_reset)) < 1:\n",
    "            # Print when we reach the end of an example (e.g. the end of a dialogue or a document)\n",
    "            # Knowing when the training procedure reaches the end is useful for diagnosing training problems\n",
    "            #print 'END-OF-BATCH EXAMPLE!'\n",
    "            is_end_of_batch = True\n",
    "\n",
    "        if state['use_nce']:\n",
    "            y_neg = rng.choice(size=(10, max_length, x_data.shape[1]), a=model.idim, p=model.noise_probs).astype('int32')\n",
    "            c, kl_divergence_cost, posterior_mean_variance = train_batch(x_data, x_data_reversed, y_neg, max_length, x_cost_mask, x_reset, ran_cost_utterance, ran_decoder_drop_mask)\n",
    "        else:\n",
    "            c, kl_divergence_cost, posterior_mean_variance = train_batch(x_data, x_data_reversed, max_length, x_cost_mask, x_reset, ran_cost_utterance, ran_decoder_drop_mask)\n",
    "\n",
    "        # Print batch statistics\n",
    "        print ('cost_sum', c)\n",
    "        print ('cost_mean', c / float(numpy.sum(x_cost_mask)))\n",
    "        print ('kl_divergence_cost_sum', kl_divergence_cost)\n",
    "        print ('kl_divergence_cost_mean', kl_divergence_cost / \n",
    "               float(len(numpy.where(x_data == model.eos_sym)[0])))\n",
    "        print ('posterior_mean_variance', posterior_mean_variance)\n",
    "\n",
    "\n",
    "        if numpy.isinf(c) or numpy.isnan(c):\n",
    "            logger.warn(\"Got NaN cost .. skipping\")\n",
    "            gc.collect()\n",
    "            continue\n",
    "\n",
    "        train_cost += c\n",
    "        train_kl_divergence_cost += kl_divergence_cost\n",
    "        train_posterior_mean_variance += posterior_mean_variance\n",
    "\n",
    "        train_done += batch['num_preds']\n",
    "        train_dialogues_done += batch['num_dialogues']\n",
    "\n",
    "        this_time = time.time()\n",
    "        if step % state['train_freq'] == 0:\n",
    "            elapsed = this_time - start_time\n",
    "\n",
    "            # Keep track of training cost for the last 'train_freq' batches.\n",
    "            current_train_cost = train_cost/train_done\n",
    "            if prev_train_done >= 1 and abs(train_done - prev_train_done) > 0:\n",
    "                current_train_cost = float(train_cost - prev_train_cost)/float(train_done - prev_train_done)\n",
    "\n",
    "            if numpy.isinf(c) or numpy.isnan(c):\n",
    "                current_train_cost = 0\n",
    "\n",
    "            prev_train_cost = train_cost\n",
    "            prev_train_done = train_done\n",
    "\n",
    "            h, m, s = ConvertTimedelta(this_time - start_time)\n",
    "\n",
    "            # We need to catch exceptions due to high numbers in exp\n",
    "            try:\n",
    "                print (\".. %.2d:%.2d:%.2d %4d mb # %d bs %d maxl %d acc_cost = %.4f acc_word_perplexity = %.4f cur_cost = %.4f cur_word_perplexity = %.4f acc_mean_word_error = %.4f acc_mean_kl_divergence_cost = %.8f acc_mean_posterior_variance = %.8f\" % (h, m, s,\\\n",
    "                                 state['time_stop'] - (time.time() - start_time)/60.,\n",
    "                                 step, \n",
    "                                 batch['x'].shape[1], \n",
    "                                 batch['max_length'], \n",
    "                                 float(train_cost/train_done), \n",
    "                                 math.exp(float(train_cost/train_done)), \n",
    "                                 current_train_cost, \n",
    "                                 math.exp(current_train_cost), \n",
    "                                 float(train_misclass)/float(train_done), \n",
    "                                 float(train_kl_divergence_cost/train_done), \n",
    "                                 float(train_posterior_mean_variance/train_dialogues_done)))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        ### Inspection phase\n",
    "        # Evaluate gradient variance every 200 steps for GRU decoder\n",
    "        if state['utterance_decoder_gating'].upper() == \"GRU\":\n",
    "            if (step % 200 == 0) and (model.add_latent_gaussian_per_utterance):\n",
    "                k_eval = 10\n",
    "\n",
    "                softmax_costs = numpy.zeros((k_eval), dtype='float32')\n",
    "                var_costs = numpy.zeros((k_eval), dtype='float32')\n",
    "                gradients_wrt_softmax = numpy.zeros((k_eval, model.qdim_decoder, model.qdim_decoder), dtype='float32')\n",
    "                for k in range(0, k_eval):\n",
    "                    batch = add_random_variables_to_batch(model.state, model.rng, batch, None, False)\n",
    "                    ran_cost_utterance = batch['ran_var_constutterance']\n",
    "                    ran_decoder_drop_mask = batch['ran_decoder_drop_mask']\n",
    "                    softmax_cost, var_cost, grads_wrt_softmax, grads_wrt_kl_divergence_cost = eval_grads(x_data, x_data_reversed, max_length, x_cost_mask, x_reset, ran_cost_utterance, ran_decoder_drop_mask)\n",
    "                    softmax_costs[k] = softmax_cost\n",
    "                    var_costs[k] = var_cost\n",
    "                    gradients_wrt_softmax[k, :, :] = grads_wrt_softmax\n",
    "\n",
    "                print ('mean softmax_costs', numpy.mean(softmax_costs))\n",
    "                print ('std softmax_costs', numpy.std(softmax_costs))\n",
    "\n",
    "                print ('mean var_costs', numpy.mean(var_costs))\n",
    "                print ('std var_costs', numpy.std(var_costs))\n",
    "\n",
    "                print ('mean gradients_wrt_softmax', \n",
    "                       numpy.mean(numpy.abs(numpy.mean(gradients_wrt_softmax, axis=0))), \n",
    "                       numpy.mean(gradients_wrt_softmax, axis=0))\n",
    "                print ('std gradients_wrt_softmax', \n",
    "                       numpy.mean(numpy.std(gradients_wrt_softmax, axis=0)), \n",
    "                       numpy.std(gradients_wrt_softmax, axis=0))\n",
    "\n",
    "\n",
    "                print ('std greater than mean', \n",
    "                       numpy.where(numpy.std(gradients_wrt_softmax, axis=0) > \n",
    "                                   numpy.abs(numpy.mean(gradients_wrt_softmax, axis=0)))[0].shape[0])\n",
    "\n",
    "                Wd_s_q = model.utterance_decoder.Wd_s_q.get_value()\n",
    "\n",
    "                print ('Wd_s_q all', numpy.sum(numpy.abs(Wd_s_q)), numpy.mean(numpy.abs(Wd_s_q)))\n",
    "                print ('Wd_s_q latent', numpy.sum(numpy.abs(\n",
    "                    Wd_s_q[(Wd_s_q.shape[0]-state['latent_gaussian_per_utterance_dim']):Wd_s_q.shape[0], :])), \n",
    "                       numpy.mean(numpy.abs(Wd_s_q[(Wd_s_q.shape[0]-state['latent_gaussian_per_utterance_dim']):Wd_s_q.shape[0], :])))\n",
    "\n",
    "                print ('Wd_s_q ratio', (numpy.sum(numpy.abs(\n",
    "                    Wd_s_q[(Wd_s_q.shape[0]-state['latent_gaussian_per_utterance_dim']):Wd_s_q.shape[0], :])) / numpy.sum(numpy.abs(Wd_s_q))))\n",
    "\n",
    "                if 'latent_gaussian_linear_dynamics' in state:\n",
    "                    if state['latent_gaussian_linear_dynamics']:\n",
    "                       prior_Wl_linear_dynamics = model.latent_utterance_variable_prior_encoder.Wl_linear_dynamics.get_value()\n",
    "                       print ('prior_Wl_linear_dynamics', numpy.sum(numpy.abs(prior_Wl_linear_dynamics)), numpy.mean(numpy.abs(prior_Wl_linear_dynamics)), \n",
    "                              numpy.std(numpy.abs(prior_Wl_linear_dynamics)))\n",
    "\n",
    "                       approx_posterior_Wl_linear_dynamics = model.latent_utterance_variable_approx_posterior_encoder.Wl_linear_dynamics.get_value()\n",
    "                       print ('approx_posterior_Wl_linear_dynamics', numpy.sum(numpy.abs(approx_posterior_Wl_linear_dynamics)),\n",
    "                              numpy.mean(numpy.abs(approx_posterior_Wl_linear_dynamics)), \n",
    "                              numpy.std(numpy.abs(approx_posterior_Wl_linear_dynamics)))\n",
    "\n",
    "                #print 'grads_wrt_softmax', grads_wrt_softmax.shape, numpy.sum(numpy.abs(grads_wrt_softmax)), numpy.abs(grads_wrt_softmax[0:5,0:5])\n",
    "                #print 'grads_wrt_kl_divergence_cost', grads_wrt_kl_divergence_cost.shape, numpy.sum(numpy.abs(grads_wrt_kl_divergence_cost)), numpy.abs(grads_wrt_kl_divergence_cost[0:5,0:5])\n",
    "\n",
    "\n",
    "        ### Evaluation phase\n",
    "        if valid_data is not None and step % state['valid_freq'] == 0 and step > 1:\n",
    "                start_validation = True\n",
    "\n",
    "        # Only start validation loop once it's time to validate and once all previous batches have been reset\n",
    "        if start_validation and is_end_of_batch:\n",
    "                start_validation = False\n",
    "                valid_data.start()\n",
    "                valid_cost = 0\n",
    "                valid_kl_divergence_cost = 0\n",
    "                valid_posterior_mean_variance = 0\n",
    "\n",
    "                valid_wordpreds_done = 0\n",
    "                valid_dialogues_done = 0\n",
    "\n",
    "\n",
    "                logger.debug(\"[VALIDATION START]\")\n",
    "\n",
    "                while True:\n",
    "                    batch = valid_data.next()\n",
    "\n",
    "                    # Validation finished\n",
    "                    if not batch:\n",
    "                        break\n",
    "                     \n",
    "                    logger.debug(\"[VALID] - Got batch %d,%d\" % (batch['x'].shape[1], batch['max_length']))\n",
    "        \n",
    "                    x_data = batch['x']\n",
    "                    x_data_reversed = batch['x_reversed']\n",
    "                    max_length = batch['max_length']\n",
    "                    x_cost_mask = batch['x_mask']\n",
    "\n",
    "                    x_reset = batch['x_reset']\n",
    "                    ran_cost_utterance = batch['ran_var_constutterance']\n",
    "                    ran_decoder_drop_mask = batch['ran_decoder_drop_mask']\n",
    "\n",
    "                    c, kl_term, c_list, kl_term_list, posterior_mean_variance = eval_batch(x_data, x_data_reversed, max_length, x_cost_mask, x_reset, ran_cost_utterance, ran_decoder_drop_mask)\n",
    "\n",
    "                    # Rehape into matrix, where rows are validation samples and columns are tokens\n",
    "                    # Note that we use max_length-1 because we don't get a cost for the first token\n",
    "                    # (the first token is always assumed to be eos)\n",
    "                    c_list = c_list.reshape((batch['x'].shape[1],max_length-1), order=(1,0))\n",
    "                    c_list = numpy.sum(c_list, axis=1)\n",
    "                    \n",
    "                    words_in_dialogues = numpy.sum(x_cost_mask, axis=0)\n",
    "                    c_list = c_list / words_in_dialogues\n",
    "                    \n",
    "\n",
    "                    if numpy.isinf(c) or numpy.isnan(c):\n",
    "                        continue\n",
    "                    \n",
    "                    valid_cost += c\n",
    "                    valid_kl_divergence_cost += kl_divergence_cost\n",
    "                    valid_posterior_mean_variance += posterior_mean_variance\n",
    "\n",
    "                    # Print batch statistics\n",
    "                    print ('valid_cost', valid_cost)\n",
    "                    print ('valid_kl_divergence_cost sample', kl_divergence_cost)\n",
    "                    print ('posterior_mean_variance', posterior_mean_variance)\n",
    "\n",
    "\n",
    "                    valid_wordpreds_done += batch['num_preds']\n",
    "                    valid_dialogues_done += batch['num_dialogues']\n",
    "\n",
    "                logger.debug(\"[VALIDATION END]\") \n",
    "                 \n",
    "                valid_cost /= valid_wordpreds_done\n",
    "                valid_kl_divergence_cost /= valid_wordpreds_done\n",
    "                valid_posterior_mean_variance /= valid_dialogues_done\n",
    "\n",
    "                if (len(timings[\"valid_cost\"]) == 0) \\\n",
    "                    or (valid_cost < numpy.min(timings[\"valid_cost\"])) \\\n",
    "                    or (save_model_on_first_valid and valid_rounds == 0):\n",
    "                    patience = state['patience']\n",
    "\n",
    "                    # Save model if there is  decrease in validation cost\n",
    "                    save(model, timings)\n",
    "                    print ('best valid_cost', valid_cost)\n",
    "                elif valid_cost >= timings[\"valid_cost\"][-1] * state['cost_threshold']:\n",
    "                    patience -= 1\n",
    "\n",
    "                if args.save_every_valid_iteration:\n",
    "                    save(model, timings, '_' + str(step) + '_')\n",
    "                if args.auto_restart:\n",
    "                    save(model, timings, '_auto_')\n",
    "\n",
    "\n",
    "                # We need to catch exceptions due to high numbers in exp\n",
    "                try:\n",
    "                    print (\"** valid cost (NLL) = %.4f, valid word-perplexity = %.4f, valid kldiv cost (per word) = %.8f, valid mean posterior variance (per word) = %.8f, patience = %d\" % (float(valid_cost), float(math.exp(valid_cost)), \n",
    "                    float(valid_kl_divergence_cost), float(valid_posterior_mean_variance), patience))\n",
    "                except:\n",
    "                    try:\n",
    "                        print (\"** valid cost (NLL) = %.4f, patience = %d\" % (float(valid_cost), patience))\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "\n",
    "                timings[\"train_cost\"].append(train_cost/train_done)\n",
    "                timings[\"train_kl_divergence_cost\"].append(train_kl_divergence_cost/train_done)\n",
    "                timings[\"train_posterior_mean_variance\"].append(train_posterior_mean_variance/train_dialogues_done)\n",
    "                timings[\"valid_cost\"].append(valid_cost)\n",
    "                timings[\"valid_kl_divergence_cost\"].append(valid_kl_divergence_cost)\n",
    "                timings[\"valid_posterior_mean_variance\"].append(valid_posterior_mean_variance)\n",
    "\n",
    "                # Reset train cost, train misclass and train done metrics\n",
    "                train_cost = 0\n",
    "                train_done = 0\n",
    "                prev_train_cost = 0\n",
    "                prev_train_done = 0\n",
    "\n",
    "                # Count number of validation rounds done so far\n",
    "                valid_rounds += 1\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    logger.debug(\"All done, exiting...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-06 21:04:33,393: __main__: DEBUG: State:\n",
      "{'add_latent_gaussian_per_utterance': True,\n",
      " 'bidirectional_utterance_encoder': False,\n",
      " 'bs': 80,\n",
      " 'collaps_to_standard_rnn': False,\n",
      " 'condition_decoder_only_on_latent_variable': False,\n",
      " 'condition_latent_variable_on_dcgm_encoder': False,\n",
      " 'condition_latent_variable_on_dialogue_encoder': True,\n",
      " 'cost_threshold': 1.003,\n",
      " 'cutoff': 1.0,\n",
      " 'decoder_bias_type': 'all',\n",
      " 'decoder_drop_previous_input_tokens': True,\n",
      " 'decoder_drop_previous_input_tokens_rate': 0.75,\n",
      " 'deep_dialogue_input': True,\n",
      " 'deep_direct_connection': False,\n",
      " 'deep_out': True,\n",
      " 'dialogue_encoder_gating': 'GRU',\n",
      " 'dialogue_rec_activation': 'lambda x: T.tanh(x)',\n",
      " 'dictionary': 'data/dataset.dict.pkl',\n",
      " 'direct_connection_between_encoders_and_decoder': False,\n",
      " 'end_sym_utterance': '</s>',\n",
      " 'eod_sym': -1,\n",
      " 'eos_sym': 1,\n",
      " 'first_speaker_sym': -1,\n",
      " 'fix_encoder_parameters': False,\n",
      " 'fix_pretrained_word_embeddings': False,\n",
      " 'initialize_from_pretrained_word_embeddings': False,\n",
      " 'kl_divergence_annealing_rate': 1.3333333333333333e-05,\n",
      " 'latent_gaussian_linear_dynamics': False,\n",
      " 'latent_gaussian_per_utterance_dim': 100,\n",
      " 'level': 'DEBUG',\n",
      " 'loop_iters': 3000000,\n",
      " 'lr': 0.0002,\n",
      " 'max_grad_steps': 80,\n",
      " 'maxout_out': False,\n",
      " 'minerr': -1,\n",
      " 'minor_speaker_sym': -1,\n",
      " 'off_screen_sym': -1,\n",
      " 'oov': '<unk>',\n",
      " 'patience': 20,\n",
      " 'pause_sym': -1,\n",
      " 'prefix': 'UbuntuModel_',\n",
      " 'pretrained_word_embeddings_file': '',\n",
      " 'qdim_decoder': 500,\n",
      " 'qdim_encoder': 500,\n",
      " 'rankdim': 300,\n",
      " 'reset_hidden_states_between_subsequences': False,\n",
      " 'reset_utterance_decoder_at_end_of_utterance': True,\n",
      " 'reset_utterance_encoder_at_end_of_utterance': True,\n",
      " 'save_dir': 'Output',\n",
      " 'scale_latent_variable_variances': 0.1,\n",
      " 'sdim': 1000,\n",
      " 'second_speaker_sym': -1,\n",
      " 'seed': 1234,\n",
      " 'sent_rec_activation': 'lambda x: T.tanh(x)',\n",
      " 'sort_k_batches': 20,\n",
      " 'test_dialogues': 'data/test.dialogues.pkl',\n",
      " 'third_speaker_sym': -1,\n",
      " 'time_stop': 44640,\n",
      " 'train_dialogues': 'data/train.dialogues.pkl',\n",
      " 'train_freq': 10,\n",
      " 'train_latent_gaussians_with_kl_divergence_annealing': True,\n",
      " 'unk_sym': 0,\n",
      " 'updater': 'adam',\n",
      " 'use_nce': False,\n",
      " 'utterance_decoder_gating': 'LSTM',\n",
      " 'utterance_encoder_gating': 'GRU',\n",
      " 'valid_dialogues': 'data/validation.dialogues.pkl',\n",
      " 'valid_freq': 5000,\n",
      " 'voice_over_sym': -1}\n",
      "2018-03-06 21:04:33,394: __main__: DEBUG: Timings:\n",
      "{'train_cost': [],\n",
      " 'train_kl_divergence_cost': [],\n",
      " 'train_misclass': [],\n",
      " 'train_posterior_mean_variance': [],\n",
      " 'valid_cost': [],\n",
      " 'valid_emi': [],\n",
      " 'valid_kl_divergence_cost': [],\n",
      " 'valid_misclass': [],\n",
      " 'valid_posterior_mean_variance': []}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-06 21:04:33,397: model: DEBUG: idim: 1009\n",
      "2018-03-06 21:04:33,398: model: DEBUG: Initializing Theano variables\n",
      "2018-03-06 21:04:33,404: model: DEBUG: Decoder bias type all\n",
      "2018-03-06 21:04:33,438: model: DEBUG: Initializing utterance encoder\n",
      "2018-03-06 21:04:33,897: model: DEBUG: Build utterance encoder\n",
      "2018-03-06 21:04:33,974: model: DEBUG: Initializing dialog encoder\n",
      "2018-03-06 21:04:36,460: model: DEBUG: Build dialog encoder\n",
      "2018-03-06 21:04:36,496: model: DEBUG: Initializing prior encoder for utterance-level latent variable\n",
      "2018-03-06 21:04:36,535: model: DEBUG: Build prior encoder for utterance-level latent variable\n",
      "2018-03-06 21:04:36,574: model: DEBUG: Initializing approximate posterior encoder for utterance-level latent variable\n",
      "2018-03-06 21:04:36,623: model: DEBUG: Build approximate posterior encoder for utterance-level latent variable\n",
      "2018-03-06 21:04:36,665: model: DEBUG: Build KL divergence cost\n",
      "2018-03-06 21:04:36,668: model: DEBUG: Initializing decoder\n",
      "2018-03-06 21:04:37,505: model: DEBUG: Build decoder (NCE)\n",
      "2018-03-06 21:04:37,575: model: DEBUG: Build decoder (EVAL)\n",
      "2018-03-06 21:04:38,917: model: DEBUG: Will train all word embeddings\n",
      "2018-03-06 21:04:39,203: __main__: DEBUG: Compile trainer\n",
      "2018-03-06 21:04:39,204: __main__: DEBUG: Training using variational lower bound on log-likelihood\n",
      "2018-03-06 21:04:39,204: model: DEBUG: Building train function\n",
      "/home/sonic/sonic/ourDirection/momus/VHRED/dialog_encdec.py:1308: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: x_data_reversed.\n",
      "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n",
      "  name=\"train_fn\")\n",
      "ERROR (theano.gof.opt): SeqOptimizer apply <theano.scan_module.scan_opt.PushOutScanOutput object at 0x7f6c7adc0400>\n",
      "2018-03-06 21:04:40,581: theano.gof.opt: ERROR: SeqOptimizer apply <theano.scan_module.scan_opt.PushOutScanOutput object at 0x7f6c7adc0400>\n",
      "ERROR (theano.gof.opt): Traceback:\n",
      "2018-03-06 21:04:40,581: theano.gof.opt: ERROR: Traceback:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\", line 242, in apply\n",
      "    sub_prof = optimizer.optimize(fgraph)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\", line 88, in optimize\n",
      "    ret = self.apply(fgraph, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 685, in apply\n",
      "    node = self.process_node(fgraph, node)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 745, in process_node\n",
      "    node, args)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 854, in push_out_inner_vars\n",
      "    add_as_nitsots)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 906, in add_nitsot_outputs\n",
      "    reason='scanOp_pushout_output')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 569, in replace_all_validate_remove\n",
      "    chk = fgraph.replace_all_validate(replacements, reason)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 543, in replace_all_validate\n",
      "    fgraph.validate()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 434, in validate_\n",
      "    ret = fgraph.execute_callbacks('validate')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/fg.py\", line 594, in execute_callbacks\n",
      "    fn(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 600, in validate\n",
      "    raise theano.gof.InconsistencyError(\"Trying to reintroduce a removed node\")\n",
      "theano.gof.fg.InconsistencyError: Trying to reintroduce a removed node\n",
      "\n",
      "2018-03-06 21:04:40,584: theano.gof.opt: ERROR: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\", line 242, in apply\n",
      "    sub_prof = optimizer.optimize(fgraph)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\", line 88, in optimize\n",
      "    ret = self.apply(fgraph, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 685, in apply\n",
      "    node = self.process_node(fgraph, node)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 745, in process_node\n",
      "    node, args)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 854, in push_out_inner_vars\n",
      "    add_as_nitsots)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 906, in add_nitsot_outputs\n",
      "    reason='scanOp_pushout_output')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 569, in replace_all_validate_remove\n",
      "    chk = fgraph.replace_all_validate(replacements, reason)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 543, in replace_all_validate\n",
      "    fgraph.validate()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 434, in validate_\n",
      "    ret = fgraph.execute_callbacks('validate')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/fg.py\", line 594, in execute_callbacks\n",
      "    fn(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 600, in validate\n",
      "    raise theano.gof.InconsistencyError(\"Trying to reintroduce a removed node\")\n",
      "theano.gof.fg.InconsistencyError: Trying to reintroduce a removed node\n",
      "\n",
      "ERROR (theano.gof.opt): SeqOptimizer apply <theano.scan_module.scan_opt.PushOutScanOutput object at 0x7f6c7adc0400>\n",
      "2018-03-06 21:04:41,128: theano.gof.opt: ERROR: SeqOptimizer apply <theano.scan_module.scan_opt.PushOutScanOutput object at 0x7f6c7adc0400>\n",
      "ERROR (theano.gof.opt): Traceback:\n",
      "2018-03-06 21:04:41,129: theano.gof.opt: ERROR: Traceback:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\", line 242, in apply\n",
      "    sub_prof = optimizer.optimize(fgraph)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\", line 88, in optimize\n",
      "    ret = self.apply(fgraph, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 685, in apply\n",
      "    node = self.process_node(fgraph, node)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 745, in process_node\n",
      "    node, args)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 854, in push_out_inner_vars\n",
      "    add_as_nitsots)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 906, in add_nitsot_outputs\n",
      "    reason='scanOp_pushout_output')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 569, in replace_all_validate_remove\n",
      "    chk = fgraph.replace_all_validate(replacements, reason)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 543, in replace_all_validate\n",
      "    fgraph.validate()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 434, in validate_\n",
      "    ret = fgraph.execute_callbacks('validate')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/fg.py\", line 594, in execute_callbacks\n",
      "    fn(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 600, in validate\n",
      "    raise theano.gof.InconsistencyError(\"Trying to reintroduce a removed node\")\n",
      "theano.gof.fg.InconsistencyError: Trying to reintroduce a removed node\n",
      "\n",
      "2018-03-06 21:04:41,130: theano.gof.opt: ERROR: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\", line 242, in apply\n",
      "    sub_prof = optimizer.optimize(fgraph)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\", line 88, in optimize\n",
      "    ret = self.apply(fgraph, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 685, in apply\n",
      "    node = self.process_node(fgraph, node)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 745, in process_node\n",
      "    node, args)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 854, in push_out_inner_vars\n",
      "    add_as_nitsots)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 906, in add_nitsot_outputs\n",
      "    reason='scanOp_pushout_output')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 569, in replace_all_validate_remove\n",
      "    chk = fgraph.replace_all_validate(replacements, reason)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 543, in replace_all_validate\n",
      "    fgraph.validate()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 434, in validate_\n",
      "    ret = fgraph.execute_callbacks('validate')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/fg.py\", line 594, in execute_callbacks\n",
      "    fn(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 600, in validate\n",
      "    raise theano.gof.InconsistencyError(\"Trying to reintroduce a removed node\")\n",
      "theano.gof.fg.InconsistencyError: Trying to reintroduce a removed node\n",
      "\n",
      "ERROR (theano.gof.opt): SeqOptimizer apply <theano.scan_module.scan_opt.PushOutScanOutput object at 0x7f6c7adc0400>\n",
      "2018-03-06 21:04:41,547: theano.gof.opt: ERROR: SeqOptimizer apply <theano.scan_module.scan_opt.PushOutScanOutput object at 0x7f6c7adc0400>\n",
      "ERROR (theano.gof.opt): Traceback:\n",
      "2018-03-06 21:04:41,548: theano.gof.opt: ERROR: Traceback:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\", line 242, in apply\n",
      "    sub_prof = optimizer.optimize(fgraph)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\", line 88, in optimize\n",
      "    ret = self.apply(fgraph, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 685, in apply\n",
      "    node = self.process_node(fgraph, node)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 745, in process_node\n",
      "    node, args)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 854, in push_out_inner_vars\n",
      "    add_as_nitsots)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 906, in add_nitsot_outputs\n",
      "    reason='scanOp_pushout_output')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 569, in replace_all_validate_remove\n",
      "    chk = fgraph.replace_all_validate(replacements, reason)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 543, in replace_all_validate\n",
      "    fgraph.validate()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 434, in validate_\n",
      "    ret = fgraph.execute_callbacks('validate')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/fg.py\", line 594, in execute_callbacks\n",
      "    fn(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 600, in validate\n",
      "    raise theano.gof.InconsistencyError(\"Trying to reintroduce a removed node\")\n",
      "theano.gof.fg.InconsistencyError: Trying to reintroduce a removed node\n",
      "\n",
      "2018-03-06 21:04:41,549: theano.gof.opt: ERROR: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\", line 242, in apply\n",
      "    sub_prof = optimizer.optimize(fgraph)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\", line 88, in optimize\n",
      "    ret = self.apply(fgraph, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 685, in apply\n",
      "    node = self.process_node(fgraph, node)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 745, in process_node\n",
      "    node, args)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 854, in push_out_inner_vars\n",
      "    add_as_nitsots)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/scan_module/scan_opt.py\", line 906, in add_nitsot_outputs\n",
      "    reason='scanOp_pushout_output')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 569, in replace_all_validate_remove\n",
      "    chk = fgraph.replace_all_validate(replacements, reason)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 543, in replace_all_validate\n",
      "    fgraph.validate()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 434, in validate_\n",
      "    ret = fgraph.execute_callbacks('validate')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/fg.py\", line 594, in execute_callbacks\n",
      "    fn(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gof/toolbox.py\", line 600, in validate\n",
      "    raise theano.gof.InconsistencyError(\"Trying to reintroduce a removed node\")\n",
      "theano.gof.fg.InconsistencyError: Trying to reintroduce a removed node\n",
      "\n",
      "2018-03-06 21:05:12,428: model: DEBUG: Building evaluation function\n",
      "/home/sonic/sonic/ourDirection/momus/VHRED/dialog_encdec.py:1355: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: x_data_reversed.\n",
      "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n",
      "  on_unused_input='warn', name=\"eval_fn\")\n",
      "2018-03-06 21:05:18,320: model: DEBUG: Building grad eval function\n",
      "/home/sonic/sonic/ourDirection/momus/VHRED/dialog_encdec.py:1365: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: x_data_reversed.\n",
      "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n",
      "  on_unused_input='warn', name=\"eval_fn\")\n",
      "/home/sonic/sonic/ourDirection/momus/VHRED/dialog_encdec.py:1365: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 4 is not part of the computational graph needed to compute the outputs: reset_mask.\n",
      "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n",
      "  on_unused_input='warn', name=\"eval_fn\")\n",
      "2018-03-06 21:05:34,874: __main__: DEBUG: Load data\n",
      "2018-03-06 21:05:34,997: SS_dataset: DEBUG: Data len is 26279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Iterator Evaluate Mode:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-06 21:05:35,003: SS_dataset: DEBUG: Data len is 1058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Iterator Evaluate Mode:  True\n",
      "W_emb = 5.5079\n",
      "kl_divergence_cost_weight = 0.0000\n",
      "W_infwd = 3.8676\n",
      "W_hhfwd = 22.3607\n",
      "b_hhfwd = 0.0000\n",
      "W_in_rfwd = 3.8794\n",
      "W_in_zfwd = 3.8711\n",
      "W_hh_rfwd = 22.3607\n",
      "W_hh_zfwd = 22.3607\n",
      "b_zfwd = 0.0000\n",
      "b_rfwd = 0.0000\n",
      "Ws_deep_input = 7.0815\n",
      "bs_deep_input = 0.0000\n",
      "Ws_in = 10.0111\n",
      "Ws_hh = 31.6228\n",
      "bs_hh = 0.0000\n",
      "Ws_in_r = 9.9965\n",
      "Ws_in_z = 10.0072\n",
      "Ws_hh_r = 31.6228\n",
      "Ws_hh_z = 31.6228\n",
      "bs_z = 0.0000\n",
      "bs_r = 0.0000\n",
      "bd_out = 0.0000\n",
      "Wd_emb = 5.5050\n",
      "Wd_hh = 22.3607\n",
      "bd_hh = 0.0000\n",
      "Wd_in = 3.8759\n",
      "Wd_s_0 = 10.5096\n",
      "bd_s_0 = 0.0000\n",
      "Wd_in_i = 3.8823\n",
      "Wd_hh_i = 4.9930\n",
      "Wd_c_i = 5.0029\n",
      "bd_i = 0.0000\n",
      "Wd_in_f = 3.8705\n",
      "Wd_hh_f = 4.9889\n",
      "Wd_c_f = 5.0038\n",
      "bd_f = 0.0000\n",
      "Wd_in_o = 3.8702\n",
      "Wd_hh_o = 4.9965\n",
      "Wd_c_o = 4.9943\n",
      "bd_o = 0.0000\n",
      "Wd_s_i = 7.4154\n",
      "Wd_s_f = 7.4187\n",
      "Wd_s = 7.4164\n",
      "Wd_s_o = 7.4117\n",
      "Wd_out = 3.8754\n",
      "Wd_e_out = 2.9884\n",
      "bd_e_out = 0.0000\n",
      "Wd_s_out = 5.7458\n",
      "Wl_deep_inputlatent_utterance_prior = 3.1559\n",
      "bl_deep_inputlatent_utterance_prior = 0.0000\n",
      "Wl_inlatent_utterance_prior = 1.0053\n",
      "bl_inlatent_utterance_prior = 0.0000\n",
      "Wl_mean_outlatent_utterance_prior = 1.0012\n",
      "bl_mean_outlatent_utterance_prior = 0.0000\n",
      "Wl_std_outlatent_utterance_prior = 1.0146\n",
      "bl_std_outlatent_utterance_prior = 0.0000\n",
      "Wl_deep_inputlatent_utterance_approx_posterior = 3.8703\n",
      "bl_deep_inputlatent_utterance_approx_posterior = 0.0000\n",
      "Wl_inlatent_utterance_approx_posterior = 0.9938\n",
      "bl_inlatent_utterance_approx_posterior = 0.0000\n",
      "Wl_mean_outlatent_utterance_approx_posterior = 1.0081\n",
      "bl_mean_outlatent_utterance_approx_posterior = 0.0000\n",
      "Wl_std_outlatent_utterance_approx_posterior = 1.0007\n",
      "bl_std_outlatent_utterance_approx_posterior = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-06 21:05:37,906: model: DEBUG: Initializing approximate posterior encoder for utterance-level latent variable\n",
      "2018-03-06 21:05:37,920: model: DEBUG: Build approximate posterior encoder for utterance-level latent variable\n",
      "2018-03-06 21:05:37,963: model: DEBUG: Build decoder (EVAL)\n",
      "/home/sonic/sonic/ourDirection/momus/VHRED/dialog_encdec.py:1528: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: x_data_reversed.\n",
      "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n",
      "  outputs=[h, hs_complete, hd], on_unused_input='warn', name=\"encoder_fn\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Bad input argument to theano function with name \"next_probs_fn\" at index 2 (0-based).  \nBacktrace when that variable is created:\n\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-3a90f5dd889d>\", line 25, in <module>\n    main(args)\n  File \"<ipython-input-4-52161fd30268>\", line 73, in main\n    model = DialogEncoderDecoder(state)\n  File \"/home/sonic/sonic/ourDirection/momus/VHRED/dialog_encdec.py\", line 2022, in __init__\n    self.beam_source = T.lvector(\"beam_source\")\nTensorType(int64, vector) cannot store a value of dtype float64 without risking loss of precision. If you do not mind this loss, you can: 1) explicitly cast your data to int64, or 2) set \"allow_input_downcast=True\" when calling \"function\". Value: \"array([0.00221239])\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3a90f5dd889d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-52161fd30268>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"%s = %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_sampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_turns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Sampled : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sonic/ourDirection/momus/VHRED/search.py\u001b[0m in \u001b[0;36msample_apply\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mjoined_context\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mutterance_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_logic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoined_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# Convert back indices to list of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sonic/ourDirection/momus/VHRED/search.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# ... done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mnext_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_hd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_probs_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_hs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_hd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mran_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mnext_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    811\u001b[0m                         s.storage[0] = s.type.filter(\n\u001b[1;32m    812\u001b[0m                             \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m                             allow_downcast=s.allow_downcast)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/tensor/type.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, data, strict, allow_downcast)\u001b[0m\n\u001b[1;32m    138\u001b[0m                             \u001b[0;34m'\"function\". Value: \"%s\"'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                             % (self, data.dtype, self.dtype, repr(data)))\n\u001b[0;32m--> 140\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                 elif (allow_downcast is None and\n\u001b[1;32m    142\u001b[0m                         \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Bad input argument to theano function with name \"next_probs_fn\" at index 2 (0-based).  \nBacktrace when that variable is created:\n\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-3a90f5dd889d>\", line 25, in <module>\n    main(args)\n  File \"<ipython-input-4-52161fd30268>\", line 73, in main\n    model = DialogEncoderDecoder(state)\n  File \"/home/sonic/sonic/ourDirection/momus/VHRED/dialog_encdec.py\", line 2022, in __init__\n    self.beam_source = T.lvector(\"beam_source\")\nTensorType(int64, vector) cannot store a value of dtype float64 without risking loss of precision. If you do not mind this loss, you can: 1) explicitly cast your data to int64, or 2) set \"allow_input_downcast=True\" when calling \"function\". Value: \"array([0.00221239])\""
     ]
    }
   ],
   "source": [
    "theano.config.floatX = 'float32'\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Models only run with float32\n",
    "    assert(theano.config.floatX == 'float32')\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--resume\", type=str, default=\"\", help=\"Resume training from that state\")\n",
    "\n",
    "    parser.add_argument(\"--force_train_all_wordemb\", action='store_true', help=\"If true, will force the model to train all word embeddings in the encoder. This switch can be used to fine-tune a model which was trained with fixed (pretrained)  encoder word embeddings.\")\n",
    "\n",
    "    parser.add_argument(\"--save_every_valid_iteration\", action='store_true', help=\"If true, will save a unique copy of the model at every validation round.\")\n",
    "\n",
    "    parser.add_argument(\"--auto_restart\", action='store_true', help=\"If true, will maintain a copy of the current model parameters updated at every validation round. Upon initialization, the script will automatically scan the output directory and and resume training of a previous model (if such exists). This option is meant to be used for training models on clusters with hard wall-times. This option is incompatible with the \\\"resume\\\" and \\\"save_every_valid_iteration\\\" options.\")\n",
    "\n",
    "    parser.add_argument(\"--prototype\", type=str, help=\"Prototype to use (must be specified)\", default='prototype_hansard_VHRED')\n",
    "\n",
    "    parser.add_argument(\"--reinitialize-latent-variable-parameters\", action='store_true', help=\"Can be used when resuming a model. If true, will initialize all latent variable parameters randomly instead of loading them from previous model.\")\n",
    "\n",
    "    parser.add_argument(\"--reinitialize-decoder-parameters\", action='store_true', help=\"Can be used when resuming a model. If true, will initialize all parameters of the utterance decoder randomly instead of loading them from previous model.\")\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
